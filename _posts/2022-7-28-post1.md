Some Title

Some months ago I got into the habit of (re)watching VTuber kareoke streams. Naturally the streamers talk a bit, sing a bit, rinse and repeat. For people like me who just want to listen to the songs with the talks cut out (quite a few streamers I follow were Japanese and I don't understand a single bit of what they said), random strangers from the internet have kindly put down timestamps of each song's start time and song name by the comment section; but they only go through so much. Some streams don't have timestamps laid down. I made a few timestamps myself and quickly turned to finding an automatic solution.

(insert picture here)

Part of the problem I'm trying to solve is "music segmentation." It's a decently well researched area for machine learning and quite a few solutions are out there on github. I randomly stumped into [InaspeechSegmenter](https://github.com/ina-foss/inaSpeechSegmenter) on googling "music segmentation python" which had won the MIREX 2018 speech detection challenge (btw MIREX website is one of the more difficult websites I've seen to navigate and find information) and has an wrapped standalone call handle and a [pip installer](https://pypi.org/project/inaSpeechSegmenter/). 

(insert picture here)

Installing inaspeechsegmenter is as easy as a pip install. I also happen to own a NVIDIA GTX1070 to use NVIDIA's CUDA to boost inaspeechsegmenter's speed; using CUDA is also as simple as pip install tensorflow-gpu (and a whole another mess of installing NVIDIA CUDA-toolkit and CuDNN behind the scenes on windows 10). Other than inaspeechsegmenter eating a good amount of RAM (recommended 16GB RAM for linux, 24GB for windows from my experience) it spits out segmented timestamps and their classifications in 1-3 minutes (~5-15 min with 4vcores CPU)!

(insert picture here)

But these "music" tagged segmentations are too fragmented to be taken as granted. Some lyrics, especially rap segments are classified as speech; sometimes music has brief pauses and that's segmented as "no-energy" inbetween "music" timestamps; the entry/exit music many streamers put at the beginning/end of their streams are kept; the cuts are often way too sharp. But I can easily deal with these by giving some logics handling them:

(insert picture here)

first I go through the segmented timestamps in reverse and take out any "no energy" fragments that are <2 seconds long (got 2 seconds from trial and error) and bridge the neighboring fragments together if they are in the same classification category; then I go through the timestamps again and only collect any "music" segments that are 60 seconds long (for some videos this needs to be way lower to 5-20 secs) while padding them 1 second before and 2 seconds after to make the cuts smoother; next I go through the curated timestamps again in reverse and check if any of them are within 5 seconds apart and merge them together (for some videos this value need to be longer, but longer may merge 2 songs together); lastly I take out any timestamps that are at the immediate beginning, end of the stream, or are less than 80 seconds long, and return the rest in human readable formats as HH:MM:SS. This is effectively [written here](https://github.com/lovegaoshi/ipynb/blob/a578a48e5e81f91adff76b8b6fbd30a83ad35bd6/inaseg.py#L16). The timestamps are much cleaner now with occasionally too fragmented cuts and too long cuts, but I can live with this. Comparing to timestamps made by other people, inaspeechsegmenter typically gets them right.

(insert picture here)

Next I simply use a command template to call ffmpeg to cut the stream into individual songs. Labeling these songs is quite difficult as I've not listened to most of the songs before and some streamers don't label what songs are what; I need to rely on song recongization tools, for example google made one, Apple has Shazam, Tencent has one as well. However google's is only available on phones; the only easily accessible and automatable software I can find is Shazam. And luckily it's also reverse engineered and wrapped in a nice pip package named [ShazamAPI](https://pypi.org/project/ShazamAPI/). Using ShazamAPI is as easy as 1-2-3; and Shazam is nice enough to not limit up to 4 concurrent queries and # of queries per month! I made my own [threaded shazam wrapper](https://github.com/lovegaoshi/ipynb/blob/a578a48e5e81f91adff76b8b6fbd30a83ad35bd6/inaseg.py#L371).

(insert picture here)

After some months of trial and error, I found Shazam does a poor job identifying songs with pitch altered; but this is much better than me holding up my phone or running an android simulator side by side. It's running so nicely that I converted the initial jupyter notebook containing this code into a [google colab](https://github.com/lovegaoshi/ipynb/blob/main/inaseg_google_colab.ipynb) and that works (and quite fast with colab's free gpu! well, slow in download); I made a docker container at [gaoshi/inaseg:gpu](https://hub.docker.com/layers/inaseg/gaoshi/inaseg/gpu/images/sha256-baa580c10b1c65778c699842882fa914123b4c70ede47e768b00da4f087118ee?context=explore) and that works; I signed up some free google trial dollars and that docker container works on google cloud's highmem N2 node; I wrapped a bilibili (effectively Youtube in China) [uploader](https://github.com/ForgQi/biliup-rs/releases) to my purposes and that works quite reliably uploading whatever I segmented to my bilibili account. After 300 USD of google dollars I've processed and uploaded 1000+ hours of segmented and labeled music. Thanks to this I currently have almost 30k songs in my hard drive that will last me 80 days listening 24X7. I've successfully turned myself into a data hoarder.

(insert picture here)
